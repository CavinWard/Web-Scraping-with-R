---
title: "Web Scraping with R & rvest"
subtitle: ""  
author: 
  - "Dr. Matthew Hendrickson"
date: "July 9, 2020"
output:
  revealjs::revealjs_presentation:
    theme: night
    center: true
    widescreen: true
    incremental: true
    fig_width: 9
    fig_height: 3
    df_print: paged
---

```{r setup, include=FALSE, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, comment = "#>", collapse = TRUE)
library(tidyverse)
library(robotstxt)
library(rvest)
```

# ITEMS TO FIX:

1. Author - may not be possible without JS or Selenium
2. Data Assembly - uneven number of records





# Topics

Ensure these are updated

1. About Me
2. A Little About Web Scraping
3. Robots.txt
4. HTML Structure
5. A Little Help with CSS
6. Scraping Methods (HTML & XPATH)
7. The Setup
8. Scraping the Data
9. Assembling the Data
10. References & Resources





# About Me

<div style="float: left; width: 50%;">
<img src = "images/headshot.png" width="450" height="450">
</div>

<div style="float: left; width: 50%;">
- Social Scientist by Training
     - Psychology & Music `%>%`
     - More Psychology `%>%`
     - Law & Policy
- Professional Experience (13+ years)
     - Higher Education Analyst
     - Independent Consultant
     - Research projects, data analysis, policy development, strategy, analytics pipeline solutions
</div>





# A Little About Web Scraping

"Web scraping is the process of automatically mining data or collecting information from the World Wide Web." -- Wikipedia

Web scraping is a flexible method to extract data from the internet. It can involve extracting numerical or text data.



## Use Cases

There are many uses for web scraping, including but not limited to:

   1. Price monitoring
   2. Sentiment analysis
   3. Time series tracking and analysis
   4. Brand monitoring
   5. Market analysis
   6. Lead generation



## Robots File

Always ensure - <strong>PRIOR to scraping</strong> - that you have rights to scrape the website.

This is critical as you can be blocked from sites or even face legal action.

Good news! You can easily check with the `robotstxt` package.





# Robots.txt

Always ensure you check the robots.txt file! This assures you are not breaking the terms of service by scraping the site.

```{r robots, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
paths_allowed(paths = c("https://netflix.com/"))
```

This example shows that Netflix does not allow you to scrape their site.





# HTML Structure

<img src = "images/html-structure.png">

<p style = "font-size:65%;">Image credit: [Professor Shawn Santo](http://www2.stat.duke.edu/~fl35/teaching/440-19F/decks/webscraping2.html#3)</p>



## HTML vs CSS
"HTML is the standard markup language for creating Web pages." -- W3Schools

"CSS describes how HTML elements are to be displayed on screen, paper, or in other media." -- W3Schools





# A Little Help with CSS
If you aren't familiar with CSS, extracting parts of a website can be daunting.

[SelectorGadget](https://selectorgadget.com/) is incredibly helpful for this purpose. However, it is only available for Chrome.

<img src = "images/selector_gadget.png">

Another option is to inspect the page elements, which is available for most major browsers, including Chrome, Firefox, as developer tools.





# HTML Tags

HTML is strucutred with "tags." These tags indicate portions of the page and can be called by their structure.

There are many types of tags - here are some important ones for scraping:

- `<h1>` - header tags
- `<p>` - paragraph elements
- `<ul>` - unordered bulleted list
- `<ol>` - ordered list
- `<li>` - individual list item
- `<div>` - division
- `<table>` - table





# Web Scraping



## Scraping Methods

HTML - syntax is easier and aligns with HTML tags

XPATH - useful when the node isn't uniquely identified with CSS





# The Setup

Set up the environment to scrap the site.

```{r setup_slide, eval = FALSE}
library(tidyverse)
library(robotstxt)
library(rvest)
```

That's it! These are all the tools you'll need.





# Determine a website to scrape

It only seems appropriate to pull data from Amazon regarding R books

Ensure we can scrape the site

```{r robots_amazon, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
paths_allowed(paths = c("https://amazon.com/"))
```



We are good to scrape!





# Setting the URL

Before you can get started, you must specific the URLs to pass to the function.

```{r amazon_html, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon <- read_html("https://www.amazon.com/s?k=R&i=stripbooks&rh=n%3A283155%2Cn%3A75%2Cn%3A13983&dc&qid=1592086532&rnid=1000&ref=sr_nr_n_1")
# amazon_pg_2 <- read_html("https://www.amazon.com/s?k=R&i=stripbooks&rh=n%3A283155%2Cn%3A75%2Cn%3A13983&dc&page=2&qid=1592086539&rnid=1000&ref=sr_pg_2")
```





# Scraping Book Titles

```{r amazon_title, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes(".s-line-clamp-2") %>% 
  html_text() -> amazon_titles
head(amazon_titles)
```

The element pulls a number of breaks and blank spaces.

Let's clean this up with `str_trim`.



## The titles have a great deal of white space and breaks (`\n`), these need to be removed

```{r amazon_title_clean, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_titles <- str_trim(amazon_titles) # Removes leading & training space
head(amazon_titles)
```

This simple function returns cleaned text.





# Get the book author(s)

```{r amazon_author, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
#  html_nodes("a.a-size-base.a-link-normal") %>% 
#  html_nodes("div.a-row.a-size-base.a-color-secondary") %>% 
html_nodes(xpath = '//*[@id="search"]/div[1]/div[2]/div/span[3]/div[2]/div[3]/div/span/div/div/div[2]/div[2]/div/div[1]/div/div/div[1]/div/a[1]') %>% 
  html_text() %>% 
  str_trim() -> amazon_authors
head(amazon_authors)
```

We run into issues due to the element in which authors are placed.

We also need to split the author fields.



## Splitting the Author Field

```{r amazon_author_split, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
#amazon %>% 
#  html_nodes("a.a-size-base.a-link-normal") %>% 
#  html_nodes("div.a-row.a-size-base.a-color-secondary") %>% 
#  html_text() %>% 
#  str_trim() -> amazon_authors
#head(amazon_authors)
```





# Get the book format

```{r format, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes("a.a-size-base.a-link-normal.a-text-bold") %>% 
  html_text() %>% 
  str_trim() -> amazon_format
head(amazon_format)
```





# Get the book price

```{r price_whole, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes(".a-price-whole") %>% 
  html_text() -> amazon_price_whole
head(amazon_price_whole)
```

The price structure splits price into two elements. We must pull each and combine them into a single price.



## Get (the rest of) the book price

```{r price_fraction, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes(".a-price-fraction") %>% 
  html_text() -> amazon_price_fraction
head(amazon_price_fraction)
```



## Combine price portions

```{r price_total, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_price <- as.numeric(paste(amazon_price_whole, amazon_price_fraction, sep = ""))
head(amazon_price)
```





# Get the book rating

```{r rate, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes("i.a-icon.a-icon-star-small.a-star-small-4-5.aok-align-bottom") %>% 
  html_text() -> amazon_rating
head(amazon_rating)
```



## Let's trim this into a usable metric

```{r rate_trim, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_rating <- as.numeric(substr(amazon_rating, 1, 3))
head(amazon_rating)
```





# Get the book rating count

We'll run into the same issue as the actual rating. But first, we'll also see a number of cleaning steps are needed.

```{r rate_n, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
#  html_nodes("span.a-size-base") %>% 
  html_nodes("div.a-row.a-size-small") %>% 
  html_text() -> amazon_rate_n
amazon_rate_n <- str_trim(amazon_rate_n) # trim
#amazon_rate_n <- str_sub(amazon_rate_n, regex(?<=\s))
amazon_rate_n <- str_sub(amazon_rate_n, -5) # keep last 5 characters
amazon_rate_n <- str_trim(amazon_rate_n) # trim leading spaces
amazon_rate_n <- as.numeric(amazon_rate_n)
head(amazon_rate_n)
```





# Get the book publication date

```{r pub_dt, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes("span.a-size-base.a-color-secondary.a-text-normal") %>% 
  html_text() -> amazon_pub_dt
head(amazon_pub_dt)
```



## We need to convert this to a date to allow easier analysis

```{r pub_dt_clean, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_pub_dt <- as.Date(amazon_pub_dt, "%b %d, %Y")
head(amazon_pub_dt)
```





# We Have the Pieces

Let's assemble the file!

1. Titles
2. Authors
3. Formats
4. Prices
5. Ratings
6. Rating Counts
7. Publication Date



## Let's Check the Scrapes

```{r uneven_counts, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
length(amazon_titles)
length(amazon_authors)
length(amazon_format)
length(amazon_price)
length(amazon_rating)
length(amazon_rate_n)
length(amazon_pub_dt)
```



## Wait! What?!?

An issue with scraping is sometimes you get an uneven number of records due to missing data elements.

We can fix this (manually)!





# Fixing the Scrapes



## Titles

```{r title_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

length(amazon_titles)
```



## Authors

```{r author_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

length(amazon_authors)
```



## Formats

There is an unexpected record (20) showing a `Paperback` text not on the page.

```{r format_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_format <- amazon_format[-20]
length(amazon_format)
```



## Prices

There is an unexpected record (20) showing a price of `14.47` not on the page.

```{r price_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_price <- amazon_price[-20]
length(amazon_price)
```



## Ratings

```{r rate_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_rating %>% 
  append(values = NA, after = 1) %>% 
  append(values = NA, after = 11) -> amazon_rating
length(amazon_rating)
```



## Rating Counts

```{r rate_n_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_rate_n %>% 
  append(values = NA, after = 1) %>% 
  append(values = NA, after = 11) -> amazon_rate_n
length(amazon_rate_n)
```



## Publication Date

```{r pub_dt_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

length(amazon_pub_dt)
```



## One More Time!

```{r even_counts, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
length(amazon_titles)
length(amazon_authors)
length(amazon_format)
length(amazon_price)
length(amazon_rating)
length(amazon_rate_n)
length(amazon_pub_dt)
```





# (Finally) Assemble the Data

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
#r_books <- tibble(title = amazon_titles,
#                  author = amazon_authors,
#                  text.format = amazon_format,
#                  price = amazon_price,
#                  rating = amazon_rating,
#                  num.ratings = amazon_rate_n,
#                  publication.date = amazon_pub_dt)
#head(r_books)
```





# Thank you
<img src = "images/twitter.png" width="30" height="30">  
[\@mjhendrickson](https://twitter.com/mjhendrickson)

<img src = "images/linkedin.png" width="30" height="30">  
[matthewjhendrickson](https://www.linkedin.com/in/matthewjhendrickson/)

<img src = "images/github.png" width="30" height="30">  
[mjhendrickson](https://github.com/mjhendrickson)

</br>

[Web Scraping in R & rvest repo](https://github.com/mjhendrickson/Web-Scraping-with-R)

This talk is freely distributed under the MIT License.





# References

- Bauer V (2016). "Introduction to Web Scraping in R: Very Applied Methods Workgoup." [https://stanford.edu/~vbauer/files/teaching/VAMScrapingSlides.html](https://stanford.edu/~vbauer/files/teaching/VAMScrapingSlides.html).
- University of Cincinnati (2018). "UC Business Analytics R Programming Guide." Specifically the portion of scraping. [https://uc-r.github.io/](https://uc-r.github.io/).
- Dataquest (no date). "Tutorial: Web Scraping in R with rvest." [https://www.dataquest.io/blog/web-scraping-in-r-rvest/](https://www.dataquest.io/blog/web-scraping-in-r-rvest/).
- Im J (2019). "Web Scraping Product Data in R with rvest and purrr." [https://www.business-science.io/code-tools/2019/10/07/rvest-web-scraping.html](https://www.business-science.io/code-tools/2019/10/07/rvest-web-scraping.html).
- Kaushik S (2017). "Beginner's Guide on Web Scraping in R (using rvest) with hands-on example." [https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/](https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/).
- Perceptive Analytics on KDnuggets (2018). "A Primer on Web Scraping in R." [https://www.kdnuggets.com/2018/01/primer-web-scraping-r.html](https://www.kdnuggets.com/2018/01/primer-web-scraping-r.html).



## References continued

- Rsquared Academy (2019). "Practical Introduction to Web Scraping in R." [https://blog.rsquaredacademy.com/2019/04/11/web-scraping/](https://blog.rsquaredacademy.com/2019/04/11/web-scraping/).
- W3Schools (no date). "CSS Introduction." [https://www.w3schools.com/css/css_intro.asp](https://www.w3schools.com/css/css_intro.asp).
- W3Schools (no date). "HTML Introduction." [https://www.w3schools.com/html/html_intro.asp](https://www.w3schools.com/html/html_intro.asp).
- W3Schools (no date). "XPath Syntax." [https://www.w3schools.com/xml/xpath_syntax.asp](https://www.w3schools.com/xml/xpath_syntax.asp).
- Wikipedia (2020). "Web scraping." [https://en.wikipedia.org/wiki/Web_scraping](https://en.wikipedia.org/wiki/Web_scraping).