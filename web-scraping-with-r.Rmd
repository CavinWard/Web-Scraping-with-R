---
title: "Web Scraping with R & rvest"
subtitle: ""  
author: 
  - "Dr. Matthew Hendrickson"
date: "July 9, 2020"
output:
  revealjs::revealjs_presentation:
    theme: night
    center: true
    widescreen: true
    incremental: true
    fig_width: 9
    fig_height: 3
    df_print: paged
---

```{r setup, include=FALSE, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, comment = "#>", collapse = TRUE)
library(tidyverse)
library(robotstxt)
library(rvest)
```


```{r save files, include=FALSE, echo = FALSE, message = FALSE, warning = FALSE, eval = FALSE}
# SAVE BACKUPS OF DATA FOR EASE OF COMPILATION
#write.table(format, "data/format.txt")
#write.table(price, "data/price.txt")
#write.table(price_fraction, "data/price_fraction.txt")
#write.table(price_whole, "data/price_whole.txt")
#write.table(pub_dt, "data/pub_dt.txt")
#write.table(rate_n, "data/rate_n.txt")
#write.table(rating, "data/rating.txt")
#write.table(titles, "data/titles.txt")
#saveRDS(r_books, "data/r_books.rds")
```





# About Me

<div style="float: left; width: 50%;">
<img src = "images/headshot.png" width="450" height="450">
</div>

<div style="float: left; width: 50%;">
- Dr. Matthew Hendrickson
- Social Scientist by Training
     - Psychology & Music `%>%`
     - More Psychology `%>%`
     - Law & Policy
- Professional Experience (13+ years)
     - Higher Education Analyst
     - Independent Consultant
     - Research projects, data analysis, policy development, strategy, analytics pipeline solutions
</div>





# Topics

1. A Little About Web Scraping
2. Robots!
3. HTML & CSS
4. The Setup
5. Scraping the Data
6. Assembling the Data
7. References & Resources





# A Little About Web Scraping

"Web scraping is the process of automatically mining data or collecting information from the World Wide Web."

  -- Wikipedia

<br>
Web scraping is a flexible method to extract data from the internet. It can involve extracting numerical or text data.



## Use Cases

There are many uses for web scraping, including but not limited to:

   1. Price monitoring
   2. Sentiment analysis
   3. Time series tracking and analysis
   4. Brand monitoring
   5. Market analysis
   6. Lead generation



# Robots!

- No, not those robots!
- Always ensure - <strong>PRIOR to scraping</strong> - that you have scraping rights!
- This is <strong>critical</strong> - you can be blocked or even face legal action!



## Robots.txt

Good news! You can easily check with the `robotstxt` package.

```{r robots, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
paths_allowed(paths = c("https://netflix.com/"))
```

Netflix does not allow you to scrape their site.





# HTML & CSS
"HTML is the standard markup language for creating Web pages."

  -- W3Schools

<br>
"CSS describes how HTML elements are to be displayed on screen, paper, or in other media."

  -- W3Schools



## HTML Structure

<img src = "images/html-structure.png">

<p style = "font-size:65%;">Image credit: [Professor Shawn Santo](http://www2.stat.duke.edu/~fl35/teaching/440-19F/decks/webscraping2.html#3)</p>



## HTML Tags

HTML is structured with "tags," which indicate portions of the page and can be called by their structure.

There are many types of tags - here are some important ones for scraping:

- `<h1>`    - header tags
- `<p>`     - paragraph elements
- `<ul>`    - unordered bulleted list
- `<ol>`    - ordered list
- `<li>`    - individual list item
- `<div>`   - division
- `<table>` - table



## A Little Help with CSS
If you aren't familiar with CSS, extracting parts of a website can be daunting.

[SelectorGadget](https://selectorgadget.com/) is incredibly helpful. However, it is only available for Chrome.

<img src = "images/selector_gadget.png">

Inspect the page elements is also helpful, which is available as a developer tool for most major browsers.



## Scraping Methods

HTML - syntax is easier and aligns with HTML tags

XPATH - useful when the node isn't uniquely identified with CSS





# The Setup

Set up the environment to scrape the site.

```{r setup_slide, eval = FALSE}
library(tidyverse)
library(robotstxt)
library(rvest)
```

That's it!



## Determine a website to scrape

It only seems appropriate to pull data from R books on Amazon.

Ensure we can scrape the site.

```{r robots_amazon, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
paths_allowed(paths = c("https://amazon.com/"))
```


<br>
We are good to scrape!





# Setting the URL

Before you get started, you must specificy the URL.

Data as of `r Sys.Date()`.

```{r amazon_html, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon <- read_html("https://www.amazon.com/s?k=R&i=stripbooks&rh=n%3A283155%2Cn%3A75%2Cn%3A13983&dc&qid=1592086532&rnid=1000&ref=sr_nr_n_1")
```





# Titles

<img src = "images/r4ds_title.png">



## Scraping Book Titles

```{r title, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes(".s-line-clamp-2") %>% 
  html_text() -> titles
head(titles)
```

The element pulls a number of breaks and blank spaces.

Let's clean this up with `str_trim`.



## Removing white space and breaks (`\n`) from the Titles

```{r title_clean, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
titles <- str_trim(titles) # Removes leading & trailing space
head(titles)
```

This simple function returns cleaned text.





# Formats

<img src = "images/r4ds_format.png">


## Scraping the Book Format

```{r format, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes("a.a-size-base.a-link-normal.a-text-bold") %>% 
  html_text() -> format
head(format)
```



## Clean up book format values

```{r format_clean, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
format <- str_trim(format)
head(format)
```





# Price

<img src = "images/r4ds_price_whole.png">

<img src = "images/r4ds_price_fraction.png">



## Scraping the Book Price

The price structure splits price into two elements. We must pull each and combine them into a single price.

```{r price_whole, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes(".a-price-whole") %>% 
  html_text() -> price_whole
head(price_whole)
```



## Scraping (the rest of) the Book Price

```{r price_fraction, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes(".a-price-fraction") %>% 
  html_text() -> price_fraction
head(price_fraction)
```



## Combine Price Portions

```{r price_total, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
price <- paste(price_whole, price_fraction, sep = "")
head(price)
```



## Make it numeric

```{r price_total_numeric, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
price <- as.numeric(price)
head(price)
```





# Rating

<img src = "images/r4ds_rating.png">



## Scraping the Book Rating

```{r rate, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes("i.a-icon.a-icon-star-small.aok-align-bottom") %>% 
  html_text() -> rating
head(rating)
```



## Let's trim this into a usable metric

```{r rate_trim, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rating <- substr(rating, 1, 3) # Takes 3 characters starting at 1
head(rating)
```



## Make it numeric

```{r rate_trim_numeric, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rating <- as.numeric(rating)
head(rating)
```





# Rating Counts

<img src = "images/r4ds_rating_n.png">



## Scraping the Book Rating Count

This element is messier and we'll need a number of cleaning steps.

```{r rate_n, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes("div.a-row.a-size-small") %>% 
  html_text() -> rate_n
head(rate_n)
```



## Trim the Rating Count

```{r rate_n_trim, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rate_n <- str_trim(rate_n) # trim \n & ' '
head(rate_n)
```



## Rating Count - Substring

```{r rate_n_substring, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rate_n <- str_sub(rate_n, -5) # keep last 5 characters
head(rate_n)
```



## Trim the Rating Count (Again)

```{r rate_n_trim_2, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rate_n <- str_trim(rate_n) # trim leading spaces
head(rate_n)
```



## Set as Numeric

```{r rate_n_numeric, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rate_n <- as.numeric(rate_n)
head(rate_n)
```





# Publication Date

<img src = "images/r4ds_publication_date.png">



## Scraping the Book Publication Date

```{r pub_dt, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes("span.a-size-base.a-color-secondary.a-text-normal") %>% 
  html_text() -> pub_dt
head(pub_dt)
```



## Convert to a date

```{r pub_dt_clean, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
pub_dt <- as.Date(pub_dt, "%b %d, %Y")
head(pub_dt)
```





# We Have the Pieces

Let's assemble the file!

1. Titles
2. Formats
3. Prices
4. Ratings
5. Rating Counts
6. Publication Date



## Let's Check the Scrapes

```{r uneven_counts, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
length(titles)
length(format)
length(price)
length(rating)
length(rate_n)
length(pub_dt)
```



## Wait! What?!?

An issue with scraping is sometimes you get an uneven number of records due to missing data elements.

We can fix this!

- ...manually...





# Fixing the Scrapes



## Titles

All titles were populated and scraped accurately. However, due to multiple formats, these records must be repeated to fill the dataframe.

```{r title_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
titles %>% 
  rep(, each = 2) -> titles
length(titles)
```



## Titles

Some titles have only 1 format.

```{r title_missing_2, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
titles <- titles [-c(10, 20, 30, 34, 40)]
length(titles)
```



## Titles

Some titles have more than 2 formats.

```{r title_missing_3, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
titles %>% 
  append(values = titles[33], after = 33) %>% 
  append(values = titles[25], after = 25) %>% 
  append(values = titles[12], after = 12) %>% 
  append(values = titles[10], after = 10) %>% 
  append(values = titles[5], after = 5) -> titles
length(titles)
```



## Formats

Nothing needed here!

```{r format_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
length(format)
```



## Prices

Or here!

```{r price_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
length(price)
```



## Ratings

Some books don't have ratings.

A book only has one rating even if it has multiple formats.

We must also account for multiple formats.

```{r rate_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rating %>% 
  append(values = NA, after = 14) %>% 
  append(values = NA, after = 12) %>% 
  append(values = NA, after = 12) -> rating
length(rating)
```



## Ratings

Like titles, the ratings need to be repeated to show on the correct row.

The same corrections are done here.

```{r rate_missing_2, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rating %>% 
  rep(, each = 2) -> rating
length(rating)
```



## Ratings

Some books have only 1 format.

```{r rate_missing_3, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rating <- rating [-c(10, 20, 30, 34, 40)]
length(rating)
```



## Ratings

Some books have more than 2 formats.

```{r rate_missing_4, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rating %>% 
  append(values = rating[33], after = 33) %>% 
  append(values = rating[25], after = 25) %>% 
  append(values = rating[12], after = 12) %>% 
  append(values = rating[10], after = 10) %>% 
  append(values = rating[5], after = 5) -> rating
length(rating)
```



## Rating Counts

Not all titles have a rating.

```{r rate_n_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rate_n %>% 
  append(values = NA, after = 14) %>% 
  append(values = NA, after = 12) %>% 
  append(values = NA, after = 12) -> rate_n
length(rate_n)
```



## Rating Counts

Like titles, the ratings need to be repeated to show on the correct row.

The same corrections are done here.

```{r rate_n_missing_2, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rate_n %>% 
  rep(, each = 2) -> rate_n
length(rate_n)
```



## Rating Counts

Some books have only 1 format.

```{r rate_n_missing_3, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rate_n <- rate_n [-c(10, 20, 30, 34, 40)]
length(rate_n)
```



## Rating Counts

Some books have more than 2 formats.

```{r rate_n_missing_4, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
rate_n %>% 
  append(values = rate_n[33], after = 33) %>% 
  append(values = rate_n[25], after = 25) %>% 
  append(values = rate_n[12], after = 12) %>% 
  append(values = rate_n[10], after = 10) %>% 
  append(values = rate_n[5], after = 5) -> rate_n
length(rate_n)
```



## Publication Date

Create extra rows due to multiple book formats.

```{r pub_dt_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
pub_dt %>% 
  rep(, each = 2) -> pub_dt
length(pub_dt)
```



## Publication Date

Some books have only 1 format.

```{r pub_dt_missing_2, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
pub_dt <- pub_dt [-c(10, 20, 30, 34, 40)]
length(pub_dt)
```



## Publication Date

Some books have more than 2 formats.

```{r pub_dt_missing_3, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
pub_dt %>% 
  append(values = pub_dt[33], after = 33) %>% 
  append(values = pub_dt[25], after = 25) %>% 
  append(values = pub_dt[12], after = 12) %>% 
  append(values = pub_dt[10], after = 10) %>% 
  append(values = pub_dt[5], after = 5) -> pub_dt
length(pub_dt)
```



## One More Check!

```{r even_counts, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
length(titles)
length(format)
length(price)
length(rating)
length(rate_n)
length(pub_dt)
```





# (Finally) Assemble the Data

```{r final, echo = TRUE, message = FALSE, warning = FALSE, eval = FALSE}
r_books <- tibble(title            = titles,
                  text_format      = format,
                  price            = price,
                  rating           = rating,
                  num_ratings      = rate_n,
                  publication_date = pub_dt)
head(r_books)
```



```{r final_printed, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
r_books <- readRDS("data/r_books.rds")
head(r_books)
```





# Thank you
<img src = "images/twitter.png" width="30" height="30">  
[\@mjhendrickson](https://twitter.com/mjhendrickson)

<img src = "images/linkedin.png" width="30" height="30">  
[matthewjhendrickson](https://www.linkedin.com/in/matthewjhendrickson/)

<img src = "images/github.png" width="30" height="30">  
[mjhendrickson](https://github.com/mjhendrickson)

</br>

[Web Scraping in R & rvest repo](https://github.com/mjhendrickson/Web-Scraping-with-R)

This talk is freely distributed under the MIT License.





# References & Resources

- Bauer V (2016). "Introduction to Web Scraping in R: Very Applied Methods Workgoup." [https://stanford.edu/~vbauer/files/teaching/VAMScrapingSlides.html](https://stanford.edu/~vbauer/files/teaching/VAMScrapingSlides.html).
- University of Cincinnati (2018). "UC Business Analytics R Programming Guide." Specifically the portion of scraping. [https://uc-r.github.io/](https://uc-r.github.io/).
- Dataquest (no date). "Tutorial: Web Scraping in R with rvest." [https://www.dataquest.io/blog/web-scraping-in-r-rvest/](https://www.dataquest.io/blog/web-scraping-in-r-rvest/).
- Im J (2019). "Web Scraping Product Data in R with rvest and purrr." [https://www.business-science.io/code-tools/2019/10/07/rvest-web-scraping.html](https://www.business-science.io/code-tools/2019/10/07/rvest-web-scraping.html).
- Kaushik S (2017). "Beginner's Guide on Web Scraping in R (using rvest) with hands-on example." [https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/](https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/).



# References & Resources continued

- Perceptive Analytics on KDnuggets (2018). "A Primer on Web Scraping in R." [https://www.kdnuggets.com/2018/01/primer-web-scraping-r.html](https://www.kdnuggets.com/2018/01/primer-web-scraping-r.html).
- Rsquared Academy (2019). "Practical Introduction to Web Scraping in R." [https://blog.rsquaredacademy.com/web-scraping/](https://blog.rsquaredacademy.com/web-scraping/).
- W3Schools (no date). "CSS Introduction." [https://www.w3schools.com/css/css_intro.asp](https://www.w3schools.com/css/css_intro.asp).
- W3Schools (no date). "HTML Introduction." [https://www.w3schools.com/html/html_intro.asp](https://www.w3schools.com/html/html_intro.asp).
- W3Schools (no date). "XPath Syntax." [https://www.w3schools.com/xml/xpath_syntax.asp](https://www.w3schools.com/xml/xpath_syntax.asp).
- Wikipedia (2020). "Web scraping." [https://en.wikipedia.org/wiki/Web_scraping](https://en.wikipedia.org/wiki/Web_scraping).